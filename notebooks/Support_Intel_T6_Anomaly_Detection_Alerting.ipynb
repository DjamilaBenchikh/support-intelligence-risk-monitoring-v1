{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2412144",
   "metadata": {},
   "source": [
    "# Support Intelligence & Risk Monitoring — Anomaly Detection & Alerting (T6)\n",
    "\n",
    "This notebook demonstrates a **monitoring + alerting layer** for support/product signals.\n",
    "\n",
    "## Goal\n",
    "Detect unusual patterns such as:\n",
    "- spike in ticket volume overall or for a specific queue\n",
    "- increase in **high-priority** rate\n",
    "- sudden surge of specific tags (e.g., outages / refunds / login issues)\n",
    "\n",
    "## Important note about timestamps\n",
    "Many public ticket datasets do not contain real timestamps.  \n",
    "To demonstrate monitoring logic, we build a **pseudo-timeline**:\n",
    "- Option A: generate synthetic `created_at` dates (recommended for a portfolio demo)\n",
    "- Option B: monitor by **batch windows** (e.g., each 500 tickets = 1 interval)\n",
    "\n",
    "Outputs:\n",
    "- alert table `outputs/t6_alerts.csv`\n",
    "- plots of time series + thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4fc63",
   "metadata": {},
   "source": [
    "## 0) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de756e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a179fd",
   "metadata": {},
   "source": [
    "## 1) Load cleaned dataset (T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/processed\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"tickets_clean_en.csv\")\n",
    "PARQUET_PATH = os.path.join(DATA_DIR, \"tickets_clean_en.parquet\")\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(\"Loading CSV:\", CSV_PATH)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "elif os.path.exists(PARQUET_PATH):\n",
    "    print(\"Loading Parquet:\", PARQUET_PATH)\n",
    "    df = pd.read_parquet(PARQUET_PATH)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Run T3 first to create data/processed/tickets_clean_en.csv (recommended).\")\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ffce5",
   "metadata": {},
   "source": [
    "## 2) Build a pseudo timeline (created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset doesn't contain real timestamps, create synthetic ones.\n",
    "# This creates a realistic-looking timeline for monitoring demos.\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "n = len(df)\n",
    "\n",
    "# Create dates over ~120 days\n",
    "start = np.datetime64(\"2025-01-01\")\n",
    "days = rng.integers(0, 120, size=n)\n",
    "df[\"created_at\"] = start + days.astype(\"timedelta64[D]\")\n",
    "\n",
    "# Ensure types\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "df[\"priority_norm\"] = df[\"priority_norm\"].astype(str).str.lower().str.strip()\n",
    "df[\"queue\"] = df[\"queue\"].fillna(\"Unknown\").astype(str)\n",
    "df[\"tags_str\"] = df[\"tags_str\"].fillna(\"\").astype(str)\n",
    "\n",
    "df[[\"created_at\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac089e",
   "metadata": {},
   "source": [
    "## 3) Aggregate metrics by day\n",
    "\n",
    "We compute daily signals used for detection:\n",
    "- `tickets_total`\n",
    "- `tickets_high`\n",
    "- `high_rate`\n",
    "- top queue volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d198d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = (\n",
    "    df.groupby(df[\"created_at\"].dt.date)\n",
    "      .agg(\n",
    "          tickets_total=(\"created_at\",\"size\"),\n",
    "          tickets_high=(\"priority_norm\", lambda s: (s==\"high\").sum()),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .rename(columns={\"created_at\":\"date\"})\n",
    ")\n",
    "daily[\"date\"] = pd.to_datetime(daily[\"date\"])\n",
    "daily[\"high_rate\"] = daily[\"tickets_high\"] / daily[\"tickets_total\"]\n",
    "\n",
    "daily.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e59101",
   "metadata": {},
   "source": [
    "## 4) Simple spike detector (rolling z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_zscore(series, window=14):\n",
    "    # z-score vs rolling mean/std\n",
    "    mu = series.rolling(window, min_periods=max(3, window//3)).mean()\n",
    "    sd = series.rolling(window, min_periods=max(3, window//3)).std(ddof=0)\n",
    "    z = (series - mu) / (sd.replace(0, np.nan))\n",
    "    return z.fillna(0.0)\n",
    "\n",
    "daily[\"z_tickets\"] = rolling_zscore(daily[\"tickets_total\"], window=14)\n",
    "daily[\"z_highrate\"] = rolling_zscore(daily[\"high_rate\"], window=14)\n",
    "\n",
    "# thresholds (tune)\n",
    "Z_TICKETS = 3.0\n",
    "Z_HIGHRATE = 3.0\n",
    "\n",
    "daily[\"alert_tickets_spike\"] = daily[\"z_tickets\"] >= Z_TICKETS\n",
    "daily[\"alert_highrate_spike\"] = daily[\"z_highrate\"] >= Z_HIGHRATE\n",
    "\n",
    "daily.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d0cc4",
   "metadata": {},
   "source": [
    "## 5) Queue-level monitoring (top queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_queues = df[\"queue\"].value_counts().head(6).index.tolist()\n",
    "\n",
    "q_daily = (\n",
    "    df[df[\"queue\"].isin(top_queues)]\n",
    "    .groupby([df[\"created_at\"].dt.date, \"queue\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .rename(columns={\"created_at\":\"date\"})\n",
    ")\n",
    "q_daily[\"date\"] = pd.to_datetime(q_daily[\"date\"])\n",
    "\n",
    "# pivot to wide format\n",
    "q_wide = q_daily.pivot(index=\"date\", columns=\"queue\", values=\"count\").fillna(0).sort_index()\n",
    "\n",
    "# compute z-scores per queue\n",
    "q_z = q_wide.apply(lambda s: rolling_zscore(s, window=14))\n",
    "\n",
    "# identify spikes\n",
    "QUEUE_Z = 3.5\n",
    "q_spikes = (q_z >= QUEUE_Z)\n",
    "q_spikes.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef42281",
   "metadata": {},
   "source": [
    "## 6) Tag surge monitoring (top tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode tags from tags_str (which looks like 'tagA | tagB')\n",
    "# Keep only top N tags for monitoring\n",
    "df[\"tags_list\"] = df[\"tags_str\"].apply(lambda s: [t.strip() for t in str(s).split(\"|\") if t.strip()])\n",
    "\n",
    "# explode\n",
    "tags_long = df.explode(\"tags_list\").rename(columns={\"tags_list\":\"tag\"})\n",
    "tags_long = tags_long[tags_long[\"tag\"].notna() & (tags_long[\"tag\"].astype(str).str.len() > 0)].copy()\n",
    "\n",
    "top_tags = tags_long[\"tag\"].value_counts().head(8).index.tolist()\n",
    "\n",
    "t_daily = (\n",
    "    tags_long[tags_long[\"tag\"].isin(top_tags)]\n",
    "    .groupby([tags_long[\"created_at\"].dt.date, \"tag\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .rename(columns={\"created_at\":\"date\"})\n",
    ")\n",
    "t_daily[\"date\"] = pd.to_datetime(t_daily[\"date\"])\n",
    "\n",
    "t_wide = t_daily.pivot(index=\"date\", columns=\"tag\", values=\"count\").fillna(0).sort_index()\n",
    "t_z = t_wide.apply(lambda s: rolling_zscore(s, window=14))\n",
    "\n",
    "TAG_Z = 3.5\n",
    "t_spikes = (t_z >= TAG_Z)\n",
    "t_spikes.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67d357",
   "metadata": {},
   "source": [
    "## 7) Build alerts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts = []\n",
    "\n",
    "# global alerts\n",
    "for _, r in daily.iterrows():\n",
    "    if r[\"alert_tickets_spike\"]:\n",
    "        alerts.append({\"date\": r[\"date\"], \"signal\": \"tickets_total\", \"value\": float(r[\"tickets_total\"]), \"z\": float(r[\"z_tickets\"]), \"level\": \"global\"})\n",
    "    if r[\"alert_highrate_spike\"]:\n",
    "        alerts.append({\"date\": r[\"date\"], \"signal\": \"high_rate\", \"value\": float(r[\"high_rate\"]), \"z\": float(r[\"z_highrate\"]), \"level\": \"global\"})\n",
    "\n",
    "# queue alerts\n",
    "for d in q_spikes.index:\n",
    "    for q in q_spikes.columns:\n",
    "        if bool(q_spikes.loc[d, q]):\n",
    "            alerts.append({\"date\": d, \"signal\": \"queue_volume\", \"value\": float(q_wide.loc[d, q]), \"z\": float(q_z.loc[d, q]), \"level\": f\"queue:{q}\"})\n",
    "\n",
    "# tag alerts\n",
    "for d in t_spikes.index:\n",
    "    for tg in t_spikes.columns:\n",
    "        if bool(t_spikes.loc[d, tg]):\n",
    "            alerts.append({\"date\": d, \"signal\": \"tag_volume\", \"value\": float(t_wide.loc[d, tg]), \"z\": float(t_z.loc[d, tg]), \"level\": f\"tag:{tg}\"})\n",
    "\n",
    "alerts_df = pd.DataFrame(alerts).sort_values([\"date\",\"signal\",\"level\"])\n",
    "print(\"Alerts:\", len(alerts_df))\n",
    "alerts_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d16386",
   "metadata": {},
   "source": [
    "## 8) Save alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "out_csv = os.path.join(OUT_DIR, \"t6_alerts.csv\")\n",
    "alerts_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"Saved alerts:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a736d",
   "metadata": {},
   "source": [
    "## 9) Plot — Global signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickets_total with z-score threshold markers\n",
    "fig, ax = plt.subplots(figsize=(11, 4.8))\n",
    "ax.plot(daily[\"date\"], daily[\"tickets_total\"], marker=\"o\", linewidth=2)\n",
    "ax.set_title(\"Daily Ticket Volume (Global)\")\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"tickets_total\")\n",
    "\n",
    "# mark spikes\n",
    "spike_dates = daily.loc[daily[\"alert_tickets_spike\"], \"date\"]\n",
    "spike_vals = daily.loc[daily[\"alert_tickets_spike\"], \"tickets_total\"]\n",
    "ax.scatter(spike_dates, spike_vals, s=80, label=f\"spike (z≥{Z_TICKETS})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# high_rate\n",
    "fig, ax = plt.subplots(figsize=(11, 4.8))\n",
    "ax.plot(daily[\"date\"], daily[\"high_rate\"], marker=\"o\", linewidth=2)\n",
    "ax.set_title(\"Daily High-Priority Rate (Global)\")\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"high_rate\")\n",
    "\n",
    "spike_dates = daily.loc[daily[\"alert_highrate_spike\"], \"date\"]\n",
    "spike_vals = daily.loc[daily[\"alert_highrate_spike\"], \"high_rate\"]\n",
    "ax.scatter(spike_dates, spike_vals, s=80, label=f\"spike (z≥{Z_HIGHRATE})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ce4f6",
   "metadata": {},
   "source": [
    "## 10) Plot — Queue volumes (top queues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in top_queues:\n",
    "    fig, ax = plt.subplots(figsize=(11, 3.8))\n",
    "    ax.plot(q_wide.index, q_wide[q], marker=\"o\", linewidth=2)\n",
    "    ax.set_title(f\"Daily Queue Volume — {q}\")\n",
    "    ax.set_xlabel(\"date\")\n",
    "    ax.set_ylabel(\"ticket count\")\n",
    "\n",
    "    spike_idx = q_spikes.index[q_spikes[q]]\n",
    "    if len(spike_idx) > 0:\n",
    "        ax.scatter(spike_idx, q_wide.loc[spike_idx, q], s=70, label=f\"spike (z≥{QUEUE_Z})\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468d666",
   "metadata": {},
   "source": [
    "## 11) Interpretation (publication-ready)\n",
    "\n",
    "- **Global volume spikes** can indicate incidents/outages or releases causing user friction.\n",
    "- **High-rate spikes** indicate a change in severity mix (possible production incident).\n",
    "- **Queue/tag spikes** localize the issue (e.g., login failures, billing refunds).\n",
    "- Thresholds should be tuned to the organization’s tolerance for missed incidents vs alert fatigue.\n",
    "\n",
    "Next: integrate this logic into a scheduled job (e.g., daily batch) that writes alerts to a DB and triggers notifications.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}