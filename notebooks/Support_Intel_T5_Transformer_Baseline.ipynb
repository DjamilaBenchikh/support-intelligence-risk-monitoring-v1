{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b27e803",
   "metadata": {},
   "source": [
    "# Support Intelligence & Risk Monitoring â€” Transformer Baseline (T5)\n",
    "\n",
    "This notebook adds a **lightweight Transformer baseline** to complement the TFâ€‘IDF models from T4.\n",
    "\n",
    "## Objectives\n",
    "- Train a **DistilBERT** (or similar) model for **priority prediction** (`low/medium/high`)\n",
    "- Compare against the **T4 baseline** (F1-macro + confusion matrix)\n",
    "- Export an artifact ready for later API inference\n",
    "\n",
    "## Notes\n",
    "- If you run locally and don't have the libraries, use the install cell below.\n",
    "- For faster execution, consider running this notebook on **Kaggle** or with a GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8960e7b",
   "metadata": {},
   "source": [
    "## 0) (Optional) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff308269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running locally and these packages are missing, uncomment:\n",
    "# !python -m pip install -U transformers datasets accelerate evaluate scikit-learn torch\n",
    "\n",
    "# Tip: On Windows, Python 3.12 is often the most compatible for ML tooling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2b448",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996bdb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.4.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.13)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\hp\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\tractosearch-0.0.1a5-py3.13.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ce3210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d4afa",
   "metadata": {},
   "source": [
    "## 2) Load cleaned dataset (T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1776c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV: data/processed\\tickets_clean_en.csv\n",
      "Loaded shape: (16338, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>answer</th>\n",
       "      <th>type</th>\n",
       "      <th>queue</th>\n",
       "      <th>priority</th>\n",
       "      <th>language</th>\n",
       "      <th>version</th>\n",
       "      <th>subject_clean</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>subject_filled</th>\n",
       "      <th>message</th>\n",
       "      <th>tags</th>\n",
       "      <th>tags_str</th>\n",
       "      <th>n_tags</th>\n",
       "      <th>body_len</th>\n",
       "      <th>message_len</th>\n",
       "      <th>is_very_short</th>\n",
       "      <th>priority_norm</th>\n",
       "      <th>category_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Account Disruption</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI am writing to...</td>\n",
       "      <td>Thank you for reaching out, &lt;name&gt;. We are awa...</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>high</td>\n",
       "      <td>en</td>\n",
       "      <td>51</td>\n",
       "      <td>Account Disruption</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI am writing to...</td>\n",
       "      <td>...</td>\n",
       "      <td>Account Disruption</td>\n",
       "      <td>Account Disruption | Dear Customer Support Tea...</td>\n",
       "      <td>['Account', 'Disruption', 'Outage', 'IT', 'Tec...</td>\n",
       "      <td>Account | Disruption | Outage | IT | Tech Support</td>\n",
       "      <td>5</td>\n",
       "      <td>544</td>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "      <td>high</td>\n",
       "      <td>Account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Query About Smart Home System Integration Feat...</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI hope this mes...</td>\n",
       "      <td>Thank you for your inquiry. Our products suppo...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Returns and Exchanges</td>\n",
       "      <td>medium</td>\n",
       "      <td>en</td>\n",
       "      <td>51</td>\n",
       "      <td>Query About Smart Home System Integration Feat...</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI hope this mes...</td>\n",
       "      <td>...</td>\n",
       "      <td>Query About Smart Home System Integration Feat...</td>\n",
       "      <td>Query About Smart Home System Integration Feat...</td>\n",
       "      <td>['Product', 'Feature', 'Tech Support']</td>\n",
       "      <td>Product | Feature | Tech Support</td>\n",
       "      <td>3</td>\n",
       "      <td>534</td>\n",
       "      <td>587</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inquiry Regarding Invoice Details</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI hope this mes...</td>\n",
       "      <td>We appreciate you reaching out with your billi...</td>\n",
       "      <td>Request</td>\n",
       "      <td>Billing and Payments</td>\n",
       "      <td>low</td>\n",
       "      <td>en</td>\n",
       "      <td>51</td>\n",
       "      <td>Inquiry Regarding Invoice Details</td>\n",
       "      <td>Dear Customer Support Team,\\n\\nI hope this mes...</td>\n",
       "      <td>...</td>\n",
       "      <td>Inquiry Regarding Invoice Details</td>\n",
       "      <td>Inquiry Regarding Invoice Details | Dear Custo...</td>\n",
       "      <td>['Billing', 'Payment', 'Account', 'Documentati...</td>\n",
       "      <td>Billing | Payment | Account | Documentation | ...</td>\n",
       "      <td>5</td>\n",
       "      <td>605</td>\n",
       "      <td>641</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Billing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             subject  \\\n",
       "0                                 Account Disruption   \n",
       "1  Query About Smart Home System Integration Feat...   \n",
       "2                  Inquiry Regarding Invoice Details   \n",
       "\n",
       "                                                body  \\\n",
       "0  Dear Customer Support Team,\\n\\nI am writing to...   \n",
       "1  Dear Customer Support Team,\\n\\nI hope this mes...   \n",
       "2  Dear Customer Support Team,\\n\\nI hope this mes...   \n",
       "\n",
       "                                              answer      type  \\\n",
       "0  Thank you for reaching out, <name>. We are awa...  Incident   \n",
       "1  Thank you for your inquiry. Our products suppo...   Request   \n",
       "2  We appreciate you reaching out with your billi...   Request   \n",
       "\n",
       "                   queue priority language  version  \\\n",
       "0      Technical Support     high       en       51   \n",
       "1  Returns and Exchanges   medium       en       51   \n",
       "2   Billing and Payments      low       en       51   \n",
       "\n",
       "                                       subject_clean  \\\n",
       "0                                 Account Disruption   \n",
       "1  Query About Smart Home System Integration Feat...   \n",
       "2                  Inquiry Regarding Invoice Details   \n",
       "\n",
       "                                          body_clean  ...  \\\n",
       "0  Dear Customer Support Team,\\n\\nI am writing to...  ...   \n",
       "1  Dear Customer Support Team,\\n\\nI hope this mes...  ...   \n",
       "2  Dear Customer Support Team,\\n\\nI hope this mes...  ...   \n",
       "\n",
       "                                      subject_filled  \\\n",
       "0                                 Account Disruption   \n",
       "1  Query About Smart Home System Integration Feat...   \n",
       "2                  Inquiry Regarding Invoice Details   \n",
       "\n",
       "                                             message  \\\n",
       "0  Account Disruption | Dear Customer Support Tea...   \n",
       "1  Query About Smart Home System Integration Feat...   \n",
       "2  Inquiry Regarding Invoice Details | Dear Custo...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['Account', 'Disruption', 'Outage', 'IT', 'Tec...   \n",
       "1             ['Product', 'Feature', 'Tech Support']   \n",
       "2  ['Billing', 'Payment', 'Account', 'Documentati...   \n",
       "\n",
       "                                            tags_str n_tags  body_len  \\\n",
       "0  Account | Disruption | Outage | IT | Tech Support      5       544   \n",
       "1                   Product | Feature | Tech Support      3       534   \n",
       "2  Billing | Payment | Account | Documentation | ...      5       605   \n",
       "\n",
       "   message_len  is_very_short  priority_norm category_mapped  \n",
       "0          565              0           high         Account  \n",
       "1          587              0         medium           Other  \n",
       "2          641              0            low         Billing  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data/processed\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"tickets_clean_en.csv\")\n",
    "PARQUET_PATH = os.path.join(DATA_DIR, \"tickets_clean_en.parquet\")\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(\"Loading CSV:\", CSV_PATH)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "elif os.path.exists(PARQUET_PATH):\n",
    "    print(\"Loading Parquet:\", PARQUET_PATH)\n",
    "    df = pd.read_parquet(PARQUET_PATH)\n",
    "else:\n",
    "    raise FileNotFoundError(\"Run T3 first to create data/processed/tickets_clean_en.csv (recommended).\")\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b792d0",
   "metadata": {},
   "source": [
    "## 3) Prepare data for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "862e660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priority_norm\n",
      "medium    6618\n",
      "high      6346\n",
      "low       3374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Required columns\n",
    "for c in [\"message\", \"priority_norm\"]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing column {c}. Re-run T3.\")\n",
    "\n",
    "df[\"message\"] = df[\"message\"].fillna(\"\").astype(str)\n",
    "df = df[df[\"message\"].str.strip().str.len() > 0].copy()\n",
    "\n",
    "# Map labels -> ids\n",
    "labels = [\"low\", \"medium\", \"high\"]  # fixed order\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "df = df[df[\"priority_norm\"].isin(labels)].copy()\n",
    "df[\"label_id\"] = df[\"priority_norm\"].map(label2id).astype(int)\n",
    "\n",
    "print(df[\"priority_norm\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b944c",
   "metadata": {},
   "source": [
    "## 4) Train/Validation split\n",
    "\n",
    "We stratify by `priority_norm` for stable evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d61da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13070, 3) Val: (3268, 3)\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df[[\"message\",\"label_id\",\"priority_norm\"]],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df[\"priority_norm\"]\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Val:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65a538",
   "metadata": {},
   "source": [
    "## 5) Build HuggingFace Datasets + Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b626cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13070/13070 [00:04<00:00, 3002.14 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3268/3268 [00:01<00:00, 2595.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"  # lightweight baseline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"message\"], truncation=True, max_length=256)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"message\",\"priority_norm\"])\n",
    "val_ds   = val_ds.map(tokenize, batched=True, remove_columns=[\"message\",\"priority_norm\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188d2b5",
   "metadata": {},
   "source": [
    "## 6) Model + TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a21272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      5\u001b[0m     MODEL_NAME,\n\u001b[0;32m      6\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_labels,\n\u001b[0;32m      7\u001b[0m     id2label\u001b[38;5;241m=\u001b[39mid2label,\n\u001b[0;32m      8\u001b[0m     label2id\u001b[38;5;241m=\u001b[39mlabel2id\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Small, safe defaults (CPU-friendly). Increase batch size / epochs on GPU.\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs/t5_priority_transformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1_macro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:1791\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1791\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2313\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2312\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py:2186\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2188\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2189\u001b[0m         )\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "set_seed(RANDOM_STATE)\n",
    "\n",
    "num_labels = len(labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Small, safe defaults (CPU-friendly). Increase batch size / epochs on GPU.\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/t5_priority_transformer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc8cb3b",
   "metadata": {},
   "source": [
    "## 7) Metrics (F1-macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600d4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels_true = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1m = f1_score(labels_true, preds, average=\"macro\")\n",
    "    return {\"f1_macro\": f1m}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c531092",
   "metadata": {},
   "source": [
    "## 8) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1abaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d9840",
   "metadata": {},
   "source": [
    "## 9) Evaluate + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(val_ds)\n",
    "logits = pred.predictions\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(logits, axis=-1)\n",
    "\n",
    "print(\"Transformer Priority F1-macro:\", round(f1_score(y_true, y_pred, average='macro'), 4))\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.6, 5.8))\n",
    "ConfusionMatrixDisplay.from_predictions(\n",
    "    [id2label[i] for i in y_true],\n",
    "    [id2label[i] for i in y_pred],\n",
    "    labels=labels,\n",
    "    ax=ax,\n",
    "    values_format=\"d\"\n",
    ")\n",
    "ax.set_title(\"Priority Confusion Matrix â€” Transformer (DistilBERT)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8be3f",
   "metadata": {},
   "source": [
    "## 10) Save artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea664a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"models/t5_priority_distilbert\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Saved model + tokenizer to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20274ab",
   "metadata": {},
   "source": [
    "## 11) How to use this later (API/batch inference)\n",
    "\n",
    "At inference time, you will:\n",
    "- load tokenizer + model from `models/t5_priority_distilbert`\n",
    "- tokenize incoming `message`\n",
    "- run forward pass and take `argmax` or apply a custom threshold policy for `high`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
